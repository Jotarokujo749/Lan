from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
import time

app = Flask(__name__)
CORS(app)

def get_google_links(query, max_links=5):
    from urllib.parse import quote
    url = f"https://www.google.com/search?q={quote(query)}&hl=fr"
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, timeout=60000)
        page.wait_for_timeout(2000)
        links = []
        for a in page.query_selector_all("a"):
            href = a.get_attribute("href")
            if href and "/url?q=" in href and "google.com" not in href:
                real = href.split("/url?q=")[1].split("&")[0]
                if real not in links:
                    links.append(real)
            if len(links) >= max_links:
                break
        browser.close()
    return links

def extract_with_browser(url):
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, timeout=20000)
            page.wait_for_timeout(2000)
            content = page.content()
            browser.close()

        soup = BeautifulSoup(content, "html.parser")
        for tag in soup(["script", "style", "nav", "header", "footer", "aside", "noscript"]):
            tag.decompose()

        candidates = soup.find_all(["p", "div", "span"])
        for c in candidates:
            txt = c.get_text(strip=True)
            if 80 < len(txt) < 600 and not any(x in txt.lower() for x in ["cookie", "javascript", "newsletter", "rgpd"]):
                return txt
        return None
    except Exception as e:
        return None

def reformuler(question, fragments):
    base = f"{question.capitalize()} : "
    unique = list(dict.fromkeys(fragments))
    synthèse = " ".join(unique[:3]) + "."
    return base + synthèse

@app.route("/zamasu", methods=["POST"])
def zamasu():
    data = request.get_json()
    question = data.get("question", "")
    if not question:
        return jsonify({"response": "Tu n'as posé aucune question."})

    links = get_google_links(question)
    fragments = []

    for url in links:
        para = extract_with_browser(url)
        if para:
            fragments.append(para)
        time.sleep(1)

    if not fragments:
        return jsonify({"response": "Aucune réponse claire trouvée sur les pages visitées."})

    réponse = reformuler(question, fragments)
    return jsonify({"response": réponse})

if __name__ == "__main__":
    app.run(port=5007)
